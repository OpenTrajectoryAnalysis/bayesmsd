{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter management\n",
    "Depending on which parameters are free or kept fixed, a simple fit model can answer very different questions. For example, let's assume we want to fit a powerlaw MSD with localization error:\n",
    "\\begin{equation}\n",
    "\\text{MSD}(\\Delta t) = \\sigma^2 + Γ(\\Delta t)^\\alpha\\,.\n",
    "\\end{equation}\n",
    "\n",
    " + should localization error be the same for all dimensions or vary independently? For example for 3D data acquired by z-stacking, we would expect the error along the $z$ axis to be significantly higher than in $x$ and $y$. So maybe we should fix $\\sigma_x$ and $\\sigma_y$ to be the same, but keep $\\sigma_z$ independent?\n",
    " + maybe localization error is known externally, so we can just fix it to a constant\n",
    " + if we fix the exponent $\\alpha$, we simply measure the associated (anomalous) diffusion constant. Notably, for $\\alpha = 1$ this reduces to the optimal estimator introduced by (Vestergaard, 2014).\n",
    " + in an anisotropic medium it is conceivable that the prefactor $\\Gamma$ could be different along the Cartesian directions; can we account for that possibility, but keep $\\alpha$ the same for all the fits?\n",
    "\n",
    "All of these cases fall under the same framework of a powerlaw fit, but introduce different constraints between the parameters. Introducing and keeping track of these constraints is the topic of this tutorial.\n",
    "\n",
    "To illustrate how this is done in ``bayesmsd`` we set up an example fit. Like in the [Quickstart](00_intro.ipynb) we use ``lib.NPXFit`` on synthetic standard normal diffusion data, but here in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bayesmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(509354708)\n",
    "data = np.cumsum(np.random.normal(size=(20, 100, 3)), axis=1) # 20 trajectories, 100 frames, 3D\n",
    "fit = bayesmsd.lib.NPXFit(data, ss_order=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an overview over fit parameters\n",
    "To achieve full flexibility, ``NPXFit`` has a whole lot of parameters:\n",
    "\n",
    " + $\\sigma^2$ (localization error),\n",
    " + $\\Gamma$ (MSD prefactor), and\n",
    " + $\\alpha$ (anomalous scaling);\n",
    " + all of the above independently for each spatial dimension.\n",
    "\n",
    "For our example data set in 3D, this results in a total of 9 fit parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in fit.parameters:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course most of the time we do not need or want this high flexibility. So by default ``NPXFit`` constrains $\\Gamma$ and $\\alpha$ to be the same across dimenions. This is achieved internally by simply fixing everything to the values for dimension 0:\n",
    "```py\n",
    "fit.parameters['log(Γ) (dim 0)'].fix_to = None\n",
    "fit.parameters['log(Γ) (dim 1)'].fix_to = 'log(Γ) (dim 0)'\n",
    "fit.parameters['log(Γ) (dim 2)'].fix_to = 'log(Γ) (dim 0)'\n",
    "... # etc.\n",
    "\n",
    "fit.parameters['α (dim 0)'].fix_to = None\n",
    "fit.parameters['α (dim 1)'].fix_to = 'α (dim 0)'\n",
    "fit.parameters['α (dim 2)'].fix_to = 'α (dim 0)'\n",
    "... # etc.\n",
    "```\n",
    "This leaves only five independent fit parameters: $\\sigma_x$, $\\sigma_y$, $\\sigma_z$, $\\Gamma$, and $\\alpha$. We can check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit.independent_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since $\\Gamma$ and $\\alpha$ are just fixed to the values for dimension 0, of course they still carry that ``'(dim 0)'`` postfix. This can serve as a good reminder of what's happening behind the scenes: ``'log(Γ) (dim 0)'`` is the MSD prefactor for each dimension. So when plotting the final MSD—and summing over dimensions—we will see a prefactor of ``d*Γ``, where ``d`` is the number of spatial dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually fixing parameters\n",
    "Using the ``fix_to`` attribute, we can tie parameters together however we see fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Independent parameters in different scenarios\")\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Ex 1: same localization error across dimensions\n",
    "fit.parameters['log(σ²) (dim 1)'].fix_to = 'log(σ²) (dim 0)'\n",
    "fit.parameters['log(σ²) (dim 2)'].fix_to = 'log(σ²) (dim 0)'\n",
    "print(\"Ex 1:\", fit.independent_parameters())\n",
    "\n",
    "# Ex 2: known localization error: σ_x = 4, σ_y = 3, σ_z = 7\n",
    "fit.parameters['log(σ²) (dim 0)'].fix_to = np.log(4**2)\n",
    "fit.parameters['log(σ²) (dim 1)'].fix_to = np.log(3**2)\n",
    "fit.parameters['log(σ²) (dim 2)'].fix_to = np.log(7**2)\n",
    "print(\"Ex 2:\", fit.independent_parameters())\n",
    "\n",
    "# Ex 3: make Γ_z independent from Γ_x and Γ_y\n",
    "# We keep the fixed localization error from Ex 2.\n",
    "fit.parameters['log(Γ) (dim 2)'].fix_to = None\n",
    "print(\"Ex 3:\", fit.independent_parameters())\n",
    "\n",
    "# Finally: reset\n",
    "fit.parameters['log(σ²) (dim 0)'].fix_to = None\n",
    "fit.parameters['log(σ²) (dim 1)'].fix_to = None\n",
    "fit.parameters['log(σ²) (dim 2)'].fix_to = None\n",
    "fit.parameters[ 'log(Γ) (dim 0)'].fix_to = None\n",
    "fit.parameters[ 'log(Γ) (dim 1)'].fix_to = 'log(Γ) (dim 0)'\n",
    "fit.parameters[ 'log(Γ) (dim 2)'].fix_to = 'log(Γ) (dim 0)'\n",
    "print(\"End :\", fit.independent_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds\n",
    "Each parameter in ``fit.parameters`` has ``bounds`` that can be used to constrain that parameter to a specific interval on the real line. Bounds might be infinite (``np.inf`` or ``-np.inf``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in fit.parameters:\n",
    "    print(f\"{name:>15s} : {fit.parameters[name].bounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: functional constraints\n",
    "We saw above how ``Parameter.fix_to`` is used to fix parameters to numerical constants or the value of other parameters. What about more complicated constraints?\n",
    "\n",
    "There are two types of constraints: equality constraints of the form $f(p_1, p_2, ...) = 0$ and inequality constraints of the form $f(p_1, p_2, ...) > 0$. The treatment for these is somewhat different.\n",
    "\n",
    "Equality constraints must be solvable for one of the parameters and can then be implemented by setting ``Parameter.fix_to`` to a callable. As an example, let us assume that we know (for some reason) the total variance $\\sigma^2\\equiv\\sigma_x^2 + \\sigma_y^2 + \\sigma_z^2$ of the localization error, but are completely ignorant as to the individual dimensions. We would therefore like to enforce that all the error variances sum to $\\sigma^2$, without fixing any of them to a constant (or to each other). This can be reformulated as\n",
    "\\begin{equation}\n",
    "\\sigma_x^2 = \\sigma^2 - \\sigma_y^2 - \\sigma_z^2\\,,\n",
    "\\end{equation}\n",
    "which we then implement as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sx_fixer(parameters, total_error=5):\n",
    "    sy2 = np.exp(parameters['log(σ²) (dim 1)'])\n",
    "    sz2 = np.exp(parameters['log(σ²) (dim 2)'])\n",
    "    return np.log(np.abs(total_error - sy2 - sz2))\n",
    "\n",
    "fit.parameters['log(σ²) (dim 0)'].fix_to = sx_fixer\n",
    "fit.parameters['log(σ²) (dim 1)'].fix_to = None\n",
    "fit.parameters['log(σ²) (dim 2)'].fix_to = None\n",
    "print(fit.independent_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now $\\sigma_x$ is fixed in terms of $\\sigma_y$ and $\\sigma_z$. But what if $\\sigma_y$ becomes very large, such that $\\sigma^2 - \\sigma_y^2 - \\sigma_z^2 < 0$? Clearly we cannot allow this, so in fact we need a second constraint, this time an inequality one: $\\sigma_y^2 + \\sigma_z^2 \\leq \\sigma^2$.\n",
    "\n",
    "Inequality constraints are tricky to enforce numerically, since they introduce sharp cuts in parameter space: as long as $f(p_1, p_2, ...) > 0$, everything is perfectly fine; but as soon as $f$ switches sign we are in an infeasible region. If the boundary $f(p_1, p_2, ...) = 0$ is reasonably irregularly shaped, this can make a precise solution of the problem very costly. ``bayesmsd`` goes the less precise, but more affordable path of introducing a smooth penalty function across the boundary, such that we get three regimes:\n",
    "\n",
    " + $f < 0$ is infeasible, i.e. the likelihood function might not even be well-defined for these parameter points\n",
    " + $f > 1$ is \"perfectly fine\", i.e. no penalization will be applied\n",
    " + $f \\in [0, 1]$ is \"still fine-ish\": in this regime we subtract a smooth penalty function from the actual likelihood function, thus discouraging the fit from exploring these regions (and coming closer to $f < 0$). Specifically, the penalization is given by:\n",
    "\\begin{equation}\n",
    "\\log L \\gets \\log L - \\exp\\cot (\\pi f) \\,,\n",
    "\\end{equation}\n",
    " which smoothly crosses over to 0 at $f = 1$ and diverges for $f \\searrow 0$.\n",
    " \n",
    "By rescaling the constraint function appropriately, we can tune the crossover width to our liking. In our example of fixed total localization error it might be enough to have the penalization kick in if $\\sigma^2 - \\sigma_y^2 - \\sigma_z^2 < 10^{-5}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sy_sz_constraint(parameters, total_error=5):\n",
    "    sy2 = np.exp(parameters['log(σ²) (dim 1)'])\n",
    "    sz2 = np.exp(parameters['log(σ²) (dim 2)'])\n",
    "    return (total_error - sy2 - sz2) / 1e-5\n",
    "\n",
    "fit.constraints.append(sy_sz_constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally, we implemented an equality and an inequality constraint to enforce $\\sigma_x^2 + \\sigma_y^2 + \\sigma_z^2 = \\sigma^2 = \\text{const.}$ So let's run the fit and look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fit.run(show_progress=True)\n",
    "\n",
    "# Print parameters\n",
    "for name in ['log(σ²) (dim 0)',\n",
    "             'log(σ²) (dim 1)',\n",
    "             'log(σ²) (dim 2)',\n",
    "             'log(Γ) (dim 0)',\n",
    "             'α (dim 0)',\n",
    "            ]:\n",
    "    print(f\"{name:>15s} = {res['params'][name]:.5f}\")\n",
    "\n",
    "# Check constraint\n",
    "print()\n",
    "sum_xyz = np.sum([np.exp(res['params'][f\"log(σ²) (dim {d})\"]) for d in range(3)])\n",
    "print(f\"σx² + σy² + σz² = {sum_xyz:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the final result satisfies the constraint; so this seems to have worked!\n",
    "\n",
    "Note that this example was mainly meant to illustrate the use of equality and inequality constraints; it might not be particularly realistic. Specifically, the notable asymmetry in the inferred localization errors is due to a spontaneous symmetry breaking mechanism: we would presumably get a similarly \"good\" fit upon permutation of the localization error dimensions, since the data is symmetric across dimensions. But since they have to add to a constant, at least one of the localization errors has to be large.\n",
    "\n",
    "A more relevant use case for an inequality constraint is ``bayesmsd.lib.SplineFit`` (which is covered in more detail elsewhere): for an arbitrarily drawn MSD curve, the associated covariance matrix is not necessarily positive definite, so we have to check this by hand. The constraint function in this case is implemented in the abstract base class as ``Fit.Cpositive_constraint``, such that it is available to any derived class that might want to use it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
